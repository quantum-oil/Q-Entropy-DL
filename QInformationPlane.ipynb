{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# useful definitions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Kullback - Leibler distance may be expressed by the formula\n",
    "\n",
    "$D_{KL} [p(X)||p(Y)] = \\sum p(x) Log\\left( \\frac{p(X)}{p(Y)} \\right) $\n",
    "\n",
    "Mutual Information is a measure for how much one may learn about some random variable X given the knowledge of some other variable Y; Mutual information is defined as\n",
    "\n",
    "$ I(X,Y) = D_{KL} [p(X,Y)||p(X)p(Y)] $\n",
    "\n",
    "$I(X,Y) =   \\sum p(X,Y) Log \\left(\\frac{p(X,Y)}{p(X)p(Y)}\\right) = \\sum p(X,Y) Log \\left(\\frac{p(X|Y)}{p(X)}\\right) $\n",
    "\n",
    "$I(X,Y) = H(X) - H(X|Y) $\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generalized KL distance for Tsallis Statistics can be written as\n",
    "\n",
    "$D_{KL_q} [p(x)||p(y)] = \\sum p(x)  \\frac{\\left(\\frac{p(x)}{p(y)}\\right)^{q-1}  -1}{q-1} $\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, Generalized Mutual Information in Tsallis Statistcs becomes\n",
    "\n",
    "$ I_q(X,Y) = D_{KL_q} [p(X,Y)||p(X) p(Y)] \\\\\n",
    "D_{KL_q} [p(X,Y)||p(X) p(Y)] = \\sum p(X,Y)  \\frac{\\left(\\frac{p(X,Y)}{p(X)p(Y)}\\right)^{q-1}  -1}{q-1} $\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# setting the DNN stage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multidimensional Variables $X$ and $Y$ shall be used to designate respectively the dataset and its labels.\n",
    "\n",
    "Following Tishby's approach we shall treat each layer of the dense network as a single random variable $T_i$, since in dense networks each neuron is connected to the whole set of inputs as well as the outputs.\n",
    "\n",
    "We are interested in understanding the evolution of layer weigths in a simple Dense Neural Network using the Information Plane, where we shall plot $I(X,T) \\, x \\, I(Y,T)$ then compare to the results of plotting $I_q(X,T) \\, x \\, I_q(Y,T) $ for a representative set of $q$ values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "matplotlib.use('agg')\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL']='3'\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.datasets import reuters\n",
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.utils import np_utils\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "ntrain = 60000\n",
    "ntest = 10000\n",
    "\n",
    "#reducing dataset\n",
    "X_train = X_train[0:ntrain]\n",
    "y_train = y_train[0:ntrain]\n",
    "\n",
    "X_train = X_train.reshape(ntrain, 784)\n",
    "X_test = X_test.reshape(ntest, 784)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "# normalizing \n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "\n",
    "#one-hot encoding\n",
    "n_classes = 10\n",
    "Y_train = np_utils.to_categorical(y_train, n_classes)\n",
    "Y_test = np_utils.to_categorical(y_test, n_classes)\n",
    "\n",
    "#model\n",
    "model = Sequential()\n",
    "model.add(Dense(512, input_shape=(784,)))\n",
    "model.add(Activation('relu'))                            \n",
    "\n",
    "model.add(Dense(10))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      " - 7s - loss: 0.2662 - acc: 0.9243 - val_loss: 0.1310 - val_acc: 0.9617\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      " - 6s - loss: 0.1087 - acc: 0.9684 - val_loss: 0.0943 - val_acc: 0.9714\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      " - 6s - loss: 0.0715 - acc: 0.9793 - val_loss: 0.0828 - val_acc: 0.9746\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      " - 6s - loss: 0.0508 - acc: 0.9855 - val_loss: 0.0692 - val_acc: 0.9777\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      " - 7s - loss: 0.0371 - acc: 0.9893 - val_loss: 0.0658 - val_acc: 0.9782\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      " - 6s - loss: 0.0278 - acc: 0.9921 - val_loss: 0.0591 - val_acc: 0.9818\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      " - 6s - loss: 0.0214 - acc: 0.9942 - val_loss: 0.0620 - val_acc: 0.9813\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      " - 6s - loss: 0.0168 - acc: 0.9953 - val_loss: 0.0617 - val_acc: 0.9809\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      " - 7s - loss: 0.0129 - acc: 0.9968 - val_loss: 0.0710 - val_acc: 0.9799\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      " - 6s - loss: 0.0107 - acc: 0.9973 - val_loss: 0.0658 - val_acc: 0.9817\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      " - 6s - loss: 0.0075 - acc: 0.9983 - val_loss: 0.0649 - val_acc: 0.9817\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      " - 6s - loss: 0.0065 - acc: 0.9987 - val_loss: 0.0631 - val_acc: 0.9825\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      " - 6s - loss: 0.0071 - acc: 0.9980 - val_loss: 0.0700 - val_acc: 0.9825\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      " - 7s - loss: 0.0057 - acc: 0.9987 - val_loss: 0.0713 - val_acc: 0.9815\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      " - 6s - loss: 0.0051 - acc: 0.9988 - val_loss: 0.0691 - val_acc: 0.9821\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      " - 6s - loss: 0.0026 - acc: 0.9996 - val_loss: 0.0709 - val_acc: 0.9824\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      " - 6s - loss: 0.0061 - acc: 0.9983 - val_loss: 0.0806 - val_acc: 0.9806\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      " - 6s - loss: 0.0063 - acc: 0.9981 - val_loss: 0.0710 - val_acc: 0.9823\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      " - 6s - loss: 0.0040 - acc: 0.9987 - val_loss: 0.0765 - val_acc: 0.9805\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      " - 6s - loss: 0.0031 - acc: 0.9990 - val_loss: 0.0802 - val_acc: 0.9807\n"
     ]
    }
   ],
   "source": [
    "epocas = 20;\n",
    "caminho = os.getcwd()\n",
    "save_dir = caminho \n",
    "for i in range(0,epocas):\n",
    "    model.fit(X_train, Y_train,\n",
    "          batch_size=128, epochs=1,\n",
    "          verbose=2,\n",
    "          validation_data=(X_test, Y_test))\n",
    "    model_name = 'keras_mnistepoca'+str(i)+'.h5'\n",
    "    model_path = os.path.join(save_dir, model_name)\n",
    "    model.save(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definindo a probabilidade conjunta $P(X,Y)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PX(nbins):\n",
    "\n",
    "    hists = []\n",
    "    breaks = []\n",
    "\n",
    "    for i in range(0,len(X_train[1])): \n",
    "        hist = np.histogram(np.transpose(X_train)[i],bins = nbins);\n",
    "        hists.append(hist[0])\n",
    "        breaks.append(hist[1])\n",
    "    \n",
    "    return hists , breaks  \n",
    "\n",
    "#calculando a probabilidade conjunta\n",
    "\n",
    "def PXY(nbins):\n",
    "\n",
    "    Px, binsX =  PX(30)\n",
    "    \n",
    "    histy = np.histogram(y_train, bins=10)[0]\n",
    "    Py = histy[0]\n",
    "    binsY = histy[1]\n",
    "    \n",
    "    for i in range(0,10):\n",
    "        indices = np.where(y_train==i)\n",
    "        \n",
    "        pxdadoy =[]\n",
    "        breaks = []\n",
    "        \n",
    "        hist = np.histogram(np.transpose(X_train[indices])[i],bins = nbins);\n",
    "        pxdadoy.append(hist[0])\n",
    "        breaks.append(hist[1])\n",
    "\n",
    "        PXY.append(pxdadoy*Py[i])\n",
    "            \n",
    "    return PXY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the Markov chain\n",
    "\n",
    "$Y \\rightarrow X \\rightarrow T_i$\n",
    "\n",
    "to get\n",
    "\n",
    "$P(T_i,X) = P(T_i|X) P(X)$\n",
    "\n",
    "and\n",
    "\n",
    "$P(T_i,Y) = \\sum P(X,Y) P(T_i | X) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to get outputs of activation functions and get $T_i$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from keras import backend as K\n",
    "\n",
    "inp = model.input                                           # input placeholder\n",
    "outputs = [layer.output for layer in model.layers]          # all layer outputs\n",
    "functors = [K.function([inp, K.learning_phase()], [out]) for out in outputs]    # evaluation functions\n",
    "\n",
    "# Testing\n",
    "test = np.random.random(input_shape)[np.newaxis,...]\n",
    "layer_outs = [func([test, 1.]) for func in functors]\n",
    "print layer_outs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
